- **定期**从kb中读出所有的raw (doc、group、table)，根据文档**类型**筛选出**未进行过精简** 操作的文档，调用generate AI Introduction API后，处理后的文件**直接**存入KB中，还是放入async schedule task queue中，等待任务自动执行？

- 文档类型有：

  ```shell
  # Document types
  TYPE_DATAMAP_TABLE_MANIFEST: ClassVar[str] = "datamap_table_manifest"
  TYPE_DATAMAP_TABLE_DETAIL: ClassVar[str] = "datamap_table_detail"
  TYPE_DATAMART_DESC_DOC: ClassVar[str] = "datamart_desc_doc"
  ```

​	除此之外，还需要新增一个新的类型 **AI Introduction**，对于精简文档操作，是仅针对**datamart_desc_doc**还是所有文档类型？

- 哪里能够看到kb的具体表格，想知道kb表格的具体字段。

- 大概执行步骤：

  1. 对于datamart_kb_names中的所有mart，依次取出其中的所有需要进行精简的文档类型Document types和该文档类型的所有文档。

     ```python
     kb_name_list = [to_datamart_kb_name(mart_name) for mart_name in mart_name_list]
     # 1、遍历所有mart
     # retrive knowledge base detail from kb
     knowledge_base_details_list = kb_client.get_details_by_knowledge_base_names(
         kb_name_list, doc_type
     )
     ```
     
  2. 判断该文档是否存在精简后的AI Introduction，由于就算一个文档执行精简操作后，**源文档也不会删除**；因此无法直接查询出所有Document types不为**AI Introduction**的文档进行精简操作。而是需要对于每一个源文档（raw file）都需要判断其存不存在对应的**AI Introduction**，如果不存在，则执行精简操作；否则跳过。
  
     ```python
     # 对于每一个文档，判断其是否存在精简文档类型，将所有mart的所有精简文档都查询出来放入到set中（但是应该不需要查询出文档的detail内容，只需要查询出源文档的唯一标识名称（唯一标识主键，例如mart+document_type+file_name））
     knowledge_base_done_list = set(kb_client.get_details_by_knowledge_base_names(
           kb_name_list, "AI Introduction"
       ))
     # 逐一判断knowledge_base_details_list中的文件是否在knowledge_base_done_list中
     for file in knowledge_base_details_list:
       if file in knowledge_base_done_list:
         continue
        # 精简后的结果文件
       result_file = generate_introduction_api(file)
       
       # 保存到kb中 ? 或者放入async schedule task queue中？
       save_to_kb(result_file)
     ```
  
- 对于`gen_mid_json_direct_llm.py`文件的提取类型不应该写死为**datamart_desc_doc**？



# problem & step v2  2025.09.29

**manifest**：kb包含的表格的清单

**doc**：具体的文档内容

**details**：表格的详细描述



- **offline**任务： **定期**从kb中读出所有的raw file(doc、group、table)，根据文档**类型**筛选出**未进行过精简** 操作的文档，调用generate AI Introduction API后，将任务存放入async schedule task queue中，等待任务自动执行，文档**覆盖**掉原来的AI Introduction。
  - 文档 给 LLM 进行处理（gen_mid_json_direct_llm.py文件的操作）
    - 每个knowledge_base_name作为一个单位，所有knowledge_base_name相同的文档text_content group到一个string中
    - 将相同的knowledge_base_name合并后的文档，交给LLM进行精简处理。
  - 如果不需要进行筛选出修改后以及经过精简的文档，则全部进行generate AI Introduction API操作即可；如果需要进行筛选，如何判断**该mart在修改后未进行精简**？通过**update_time**比较（源文档的**update_time**与其对应的**AI Introduction** 的**update_time**不一致时，说明需要进行操作）？
  - 筛选出来需要进行精简操作的mart，对其调用generate AI Introduction API，得到一个mart描述的简要文档result_introduction.txt。
  - 将result_introduction.txt转换为kb所需要的类型格式KnowledgeBaseDetail，存入async schedule task queue中，实现offline更新操作。



- **online**任务：当用户主动修改topic或mart时，将kb中对应的topic进行精简操作。

  - 对指定的topic或mart进行精简操作，因此generate AI Introduction API输入参数需要支持传入**topic的列表和mart的列表**。
  - 调用API的结果esult_introduction.txt转换为kb所需要的类型格式KnowledgeBaseDetail，直接插入到表格knowledge_base_details_v1_5_4中，同时还可能需要修改表格chatbi_topic、chatbi_topic_docs、chatbi_topic_tables。

  

# problem & step v3 2025.09.30

## 扩展任务队列支持多种任务类型

### 当前队列系统的限制

当前的异步任务队列只能处理`ConvertRequest`文档转换请求，存在以下限制：

1. **类型固化**：`TaskHandler = Callable[[ConvertRequest], Awaitable[dict[str, Any]]]`
2. **处理器单一**：只能处理文档转换任务
4. **Worker逻辑固化**：硬编码调用单一处理函数

### 扩展方案：通用任务队列

#### 1. 设计通用任务基类

```python
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Dict, Optional, Union
from pydantic import BaseModel
import uuid

class TaskType(Enum):
    """任务类型枚举"""
    DOCUMENT_CONVERT = "document_convert"
    MART_DATA_PROCESS = "mart_data_process"

class BaseTaskRequest(BaseModel, ABC):
    """任务请求基类"""
    task_id: str = None
    task_type: TaskType
    metadata: Optional[Dict[str, Any]] = None
    
    def __init__(self, **data):
        if not data.get('task_id'):
            data['task_id'] = str(uuid.uuid4())
        super().__init__(**data)

class TaskResult(BaseModel):
    """任务结果统一格式"""
    task_id: str
    task_type: TaskType
    code: int  # 200成功，400客户端错误，500服务器错误
    message: str
    data: Dict[str, Any]
    execution_time: Optional[float] = None
    queue_size: Optional[int] = None
```

#### 2. 具体任务类型定义

```python
# 文档转换任务（现有）
class DocumentConvertTask(BaseTaskRequest):
    task_type: TaskType = TaskType.DOCUMENT_CONVERT
    url: str
    is_use_html: bool = True

# Mart数据处理任务
class MartDataTask(BaseTaskRequest):
    task_type: TaskType = TaskType.MART_DATA_PROCESS
    mart_name_list: List[str]
    data_content_list: Dict[str, KnowledgeBaseDetail]
```

#### 3. 通用任务处理器接口

```python
from abc import ABC, abstractmethod

class BaseTaskHandler(ABC):
    """任务处理器基类"""
    
    @abstractmethod
    async def handle(self, task: BaseTaskRequest) -> TaskResult:
        """处理任务的抽象方法"""
        pass
    
    @abstractmethod
    def can_handle(self, task_type: TaskType) -> bool:
        """判断是否能处理指定类型的任务"""
        pass
    
    def get_handler_name(self) -> str:
        """获取处理器名称"""
        return self.__class__.__name__

# 文档转换处理器
class DocumentConvertHandler(BaseTaskHandler):
    async def handle(self, task: DocumentConvertTask) -> TaskResult:
        start_time = time.time()
        try:
            # 复用现有的转换逻辑
            file_url = task.url
            
            if "docs.google.com" in file_url:
                loop = asyncio.get_event_loop()
                markdown_content = await loop.run_in_executor(
                    None, convert_to_markdown, file_url, task.is_use_html
                )
            elif "confluence.shopee.io" in file_url:
                loop = asyncio.get_event_loop()
                markdown_content = await loop.run_in_executor(
                    None, confluence_page_to_markdown, file_url
                )
            elif "datasuite.shopee.io/forum/article" in file_url:
                loop = asyncio.get_event_loop()
                markdown_content = await loop.run_in_executor(
                    None, forum_to_markdown, file_url
                )
            else:
                return TaskResult(
                    task_id=task.task_id,
                    task_type=task.task_type,
                    code=400,
                    message=f"Unsupported file type: {file_url}",
                    data={}
                )
            
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=200,
                message="Document converted successfully",
                data={"markdown": markdown_content},
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=500,
                message=str(e),
                data={},
                execution_time=execution_time
            )
    
    def can_handle(self, task_type: TaskType) -> bool:
        return task_type == TaskType.DOCUMENT_CONVERT

# Mart数据处理器
class MartDataHandler(BaseTaskHandler):
    async def handle(self, task: MartDataTask) -> TaskResult:
        start_time = time.time()
        try:
            # 数据验证
            self._validate_mart_data(task)
            
            # 批量处理MySQL
            process_result = await self._process_mysql(task)
            
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=200,
                message="Mart data processed successfully",
                data={
                    "mart_name": task.mart_name_list,
                    "processed_records": process_result.get("count", 0)
                },
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=500,
                message=f"Failed to process mart data: {str(e)}",
                data={},
                execution_time=execution_time
            )
    
    def can_handle(self, task_type: TaskType) -> bool:
        return task_type == TaskType.MART_DATA_PROCESS
    
    def _validate_mart_data(self, task: MartDataTask):
        """数据验证逻辑"""
        if not task.mart_name_list:
            raise ValueError("Mart name is required")
        if not task.data_content_list:
            raise ValueError("Data content is required")
    
    async def _batch_process_mysql(self, task: MartDataTask) -> Dict[str, Any]:
        """批量插入MySQL逻辑"""
        # 实现MySQL插入逻辑
        return {"count": 1, "status": "success"}
```

#### 4. 通用任务队列管理器

```python
class TaskQueueManager:
    """通用任务队列管理器"""
    
    def __init__(self, max_queue_size: int = 50):
        self.task_queue = asyncio.Queue(maxsize=max_queue_size)
        self.handlers: Dict[TaskType, BaseTaskHandler] = {}
        self.running = False
        self.worker_task = None
        
    def register_handler(self, handler: BaseTaskHandler):
        """注册任务处理器"""
        for task_type in TaskType:
            if handler.can_handle(task_type):
                self.handlers[task_type] = handler
                print(f"Registered handler {handler.get_handler_name()} for {task_type.value}")
    
    async def submit_task(self, task: BaseTaskRequest) -> TaskResult:
        """提交任务到队列"""
        if self.task_queue.full():
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=429,
                message="Task queue is full, please try again later",
                data={"max_queue_size": self.task_queue.maxsize, "current_queue_size": self.task_queue.qsize()}
            )
        
        # 创建Future接收结果
        future = asyncio.Future()
        
        try:
            # 将任务和Future放入队列
            self.task_queue.put_nowait((task, future))
        except asyncio.QueueFull:
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=429,
                message="Task queue is full, please try again later",
                data={}
            )
        
        try:
            # 等待处理结果
            result = await future
            result.queue_size = self.task_queue.qsize()
            return result
        except Exception as e:
            return TaskResult(
                task_id=task.task_id,
                task_type=task.task_type,
                code=500,
                message=str(e),
                data={}
            )
    
    async def worker(self):
        """通用工作线程"""
        while self.running:
            try:
                # 获取任务
                task, future = await self.task_queue.get()
                
                # 查找处理器
                handler = self.handlers.get(task.task_type)
                if not handler:
                    result = TaskResult(
                        task_id=task.task_id,
                        task_type=task.task_type,
                        code=400,
                        message=f"No handler found for task type: {task.task_type.value}",
                        data={}
                    )
                    future.set_result(result)
                    continue
                
                # 执行任务处理
                try:
                    result = await handler.handle(task)
                    future.set_result(result)
                except Exception as e:
                    result = TaskResult(
                        task_id=task.task_id,
                        task_type=task.task_type,
                        code=500,
                        message=f"Handler execution failed: {str(e)}",
                        data={}
                    )
                    future.set_result(result)
                
            except Exception as e:
                print(f"Worker error: {e}")
            finally:
                self.task_queue.task_done()
    
    async def start(self):
        """启动队列管理器"""
        self.running = True
        self.worker_task = asyncio.create_task(self.worker())
        print("Task queue manager started")
    
    async def stop(self):
        """停止队列管理器"""
        self.running = False
        if self.worker_task:
            self.worker_task.cancel()
        print("Task queue manager stopped")
```

#### 5. 集成到FastAPI应用

```python
# 全局任务队列管理器
task_manager = TaskQueueManager(max_queue_size=50)

@app.on_event("startup")
async def startup_event():
    """应用启动时初始化任务队列"""
    # 注册各种任务处理器
    task_manager.register_handler(DocumentConvertHandler())
    task_manager.register_handler(MartDataHandler())
    
    # 启动任务队列
    await task_manager.start()

@app.on_event("shutdown")
async def shutdown_event():
    """应用关闭时清理资源"""
    await task_manager.stop()

# 通用任务提交API
@app.post("/tasks/submit")
async def submit_task(task: Union[DocumentConvertTask, MartDataTask]):
    """通用任务提交接口"""
    result = await task_manager.submit_task(task)
    return result.dict()

# 保持向后兼容的文档转换API
@app.post("/convert")
async def convert_google_doc_to_markdown(request: ConvertRequest):
    """文档转换API"""
    # 转换为新的任务格式
    task = DocumentConvertTask(
        url=request.url,
        is_use_html=request.is_use_html
    )
    
    result = await task_manager.submit_task(task)
    
    # 转换为旧的响应格式
    return {
        "code": result.code,
        "message": result.message,
        "data": result.data
    }

# 新的Mart数据插入API
@app.post("/mart/process")
async def process_mart_data(task: MartDataTask):
    """Mart数据插入API"""
    result = await task_manager.submit_task(task)
    return result.dict()

# 任务状态查询API
@app.get("/tasks/status")
async def get_queue_status():
    """获取队列状态"""
    return {
        "queue_size": task_manager.task_queue.qsize(),
        "max_queue_size": task_manager.task_queue.maxsize,
        "registered_handlers": list(task_manager.handlers.keys()),
        "is_running": task_manager.running
    }
```

#### 6. 使用示例

```python
# 提交文档转换任务
doc_task = DocumentConvertTask(
    url="https://docs.google.com/document/d/xxx",
    is_use_html=True,
)
result = await task_manager.submit_task(doc_task)

# 提交Mart数据处理任务
mart_task = MartDataTask(
    mart_name="Order Mart",
    data_type="datamart_desc_doc",
    data_content={
        "knowledge_base_name": "prefill_datamart_Order Mart",
        "title": "Order Mart Introduction",
        "text_content": "精简后的内容..."
    },
)
result = await task_manager.submit_task(mart_task)

# 批量提交任务
tasks = [doc_task, mart_task]
results = []
for task in tasks:
    result = await task_manager.submit_task(task)
    results.append(result)
```

 

# problem & step 2025.10.09

## 需要的东西

- **generate AI introduction API 的输入、输出**
  - input args （既需要对mart进行精简操作，也需要对topic进行精简操作，列表内的kb_name是筛选后的结果）
    - kb_name_list : List[str]
  - output results
    - ai_introduction_list : List[ai_introduction]
- 将精简后的结果ai_introduction_list插入到kb数据库中（mysql）是否有**现有的API函数**。在online操作和offline操作中都有这个需求。
- Find DataScope任务（从知识库kb中得到数据域data domain，JAVA层面的操作，是否有现有的API函数）
  - input args
  - output results



# problem & step 2025.10.11

![llm summary](/Users/zhilong.zhang/Downloads/pictures/llm summary.jpeg)

- 对于 kb service中的 async scheduler task，在Java中使用的是 ***@Async注解+线程池***，并非一个异步任务队列。每次调用**@Async注解的函数**时（例如用户更新topic，可能需要处理文档，会调用**processDoc函数**），会自动将其放入到线程池中进行执行，而不是（将其放入任务队列中，后续经过图中我们实现的红色的**downstream process**，将任务从任务队列中取出然后经过蓝色虚线框住的一个封装好的pipeline，从而实现文档的精简入库）。因此这个***封装好的pipeline感觉无法在async scheduler task之后执行***。



download任务是从什么地方download什么数据？是feedback的数据吗？（我看您之前的文章中，是从kafka的消费者那指出到download任务），download的数据保存到什么地方？（mysql？）





```shell
GENERAL_ERROR: Object of type ndarray is not JSON serializable

An error was encountered:
Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

spark架构是什么样的？对于一个任务而言具体执行流程是什么（driver节点接收具体任务，然后分配给worker节点具体执行？），spark是如何并行执行的？如果使用了collect或show等终态函数，会将所有executor上的数据聚合到drive中吗？然后之前由k个executor进程并发完成的spark任务全部到drive程序中执行，从而失去之前的并发能力？
```



1. time指的是下载feedback时的时间戳？还是feedback info的create time或 update time？

2. message id 是response id 还是request id？

   

input:

​	topic_id : 可以转化为topic_name

​	user_email：用来checkPermission

​	filter：筛选条件  （message id、session id 、feedback Type（like or dislike)、user、start time end time

output:

​	InputStreamResource（csv文件）



根据

```java
public FeedbackDownloadDTO downloadFeedbackRecords(FeedbackDownloadRequest request) {
        // todo 1. 根据topic id 查询出 topic name，作为csv文件的命名 （或者前端直接连带topic name一起返回）
        String topicName = chatbiTopicService.getTopicById(request.getTopicId()).orElseThrow().getName();

        // todo 2. 根据筛选条件，从数据库中查询符合条件的 records ，
        //  涉及的表: topic_message_relation_tab、feedback_cycle_tab、 tracking_info_tab
        List<FeedbackDownloadDao> feedbackDownloadDaoRecords = feedbackDownloadRepository.queryFeedbackDownloadData(
          request.getTopicId(),
          request.getMessageId(),
          request.getSessionId(),
          request.getFeedback(),
          request.getUserName(),
          request.getStartTime(),
          request.getEndTime()
        );

        ByteArrayOutputStream csvStream = new ByteArrayOutputStream();
        try (PrintWriter writer = new PrintWriter(
          new OutputStreamWriter(csvStream, StandardCharsets.UTF_8))) {

            // 写BOM（防止Excel打开乱码）
            writer.write('\uFEFF');

            // 写入CSV头
            writer.println("message_id,session_id,status,comment,question,answer,feedback_result,feedback_reason,user,time");

            // 写入数据
            for (FeedbackDownloadDao feedbackDownloadDaoRecord : feedbackDownloadDaoRecords) {
                writer.printf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%d\n",
                  feedbackDownloadDaoRecord.getMessageId(),
                  feedbackDownloadDaoRecord.getSessionId(),
                  feedbackDownloadDaoRecord.getStatus(),
                  feedbackDownloadDaoRecord.getComment(),
                  feedbackDownloadDaoRecord.getQuestion(),
                  feedbackDownloadDaoRecord.getAnswer(),
                  feedbackDownloadDaoRecord.getFeedbackResult(),
                  feedbackDownloadDaoRecord.getFeedbackReason(),
                  feedbackDownloadDaoRecord.getUser(),
                  feedbackDownloadDaoRecord.getTime());
            }
        }

        // 3. 返回给前端
        ByteArrayInputStream inputStream = new ByteArrayInputStream(
          csvStream.toByteArray());

        // 获取当前时间戳
        long timeStamp = System.currentTimeMillis();

        // HTTP response 请求头
        HttpHeaders headers = new HttpHeaders();
        headers.add("Content-Disposition", String.format("attachment; filename=%s_feedback_tracking_%s.csv", topicName, timeStamp));
        headers.add("Content-Type", "text/csv; charset=UTF-8");

        return FeedbackDownloadDTO.builder()
          .headers(headers)
          .downloadResultCsv(new InputStreamResource(inputStream))
          .build();
    }
```



```java
  /**
   * 根据条件查询反馈下载数据（包含team code）
   *
   * @param topicId   话题ID（必需）
   * @param messageId 消息ID（可选）
   * @param sessionId 会话ID（可选）
   * @param teamCode  团队代码（可选）
   * @param feedback  反馈结果（可选）
   * @param user      用户名（可选）
   * @param startTime 开始时间（可选）
   * @param endTime   结束时间（可选）
   * @return 反馈下载数据列表
   */
  @Query("""
      SELECT new com.shopee.di.diana.kb.dao.entity.FeedbackDownloadDao(
        a.responseId,
        b.sessionId,
        d.name,
        a.status,
        c.comment,
        a.question,
        a.answer,
        a.feedback,
        a.feedbackReason,
        c.user,
        c.ctime
      )
      FROM FeedbackCycleDao a
      INNER JOIN TopicMessageRelationDao b ON a.responseId = b.responseId
        AND a.deletedAt = 0
        AND b.deletedAt = 0
      INNER JOIN TrackingInfoDao c ON b.responseId = c.responseId
        AND c.deletedAt = 0
      INNER JOIN ChatbiTopicDao d ON b.topicId = d.id
        AND d.deletedAt = 0
      WHERE b.topicId in :topicIds
        AND a.questionId != 0
        AND (:teamCode IS NULL OR d.teamCode = :teamCode)
        AND (:messageId IS NULL OR a.responseId = :messageId)
        AND (:sessionId IS NULL OR b.sessionId = :sessionId)
        AND (:status IS NULL OR a.status in :status)
        AND (:feedback IS NULL OR a.feedback = :feedback)
        AND (:user IS NULL OR c.user = :user)
        AND (:startTime IS NULL OR c.ctime >= :startTime)
        AND (:endTime IS NULL OR c.ctime <= :endTime)
      """)
  List<FeedbackDownloadDao> queryFeedbackDownloadData(
      @Param("topicIds") List<Long> topicIds,
      @Param("messageId") Long messageId,
      @Param("sessionId") Long sessionId,
      @Param("teamCode") String teamCode,
      @Param("status") List<String> status,
      @Param("feedback") String feedback,
      @Param("user") String user,
      @Param("startTime") Long startTime,
      @Param("endTime") Long endTime);
```

{
  "topicIds": [
    28
  ],
  "messageId": null,
  "teamCode": null,
  "status":null,
  "sessionId": null,
  "feedback": null,
  "userName": null,
  "startTime": null,
  "endTime": null
}



1. 权限判断问题
2. 复用SQL代码
3. 增加前端request字段
4. 处理 多topic 下载



```java
  /**
   * 用户点击 download 按钮，下载指定筛选条件的反馈记录到 CSV 文件
   */
  @PostMapping("/tracking/download")
//  @PermCheck(topicId = "#request.topicIds")
  @Operation(summary = "下载指定反馈记录到 CSV 文件", description = "用户点击 download 按钮下载指定筛选后的反馈记录到 CSV 文件。"
      + "返回结果包含 HTTP 响应头和反馈数据列表")
  public ResponseEntity<InputStreamResource> downloadRecords(
      @RequestBody FeedbackDownloadRequest request) {
    log.info("Feedback records download user: topicId={}, teamCode={}", request.getTopicIds(), request.getTeamCode());

    FeedbackDownloadDTO feedbackDownloadDTO = feedbackService.downloadFeedbackRecords(request);
    return ResponseEntity.ok()
        .headers(feedbackDownloadDTO.getHeaders())
        .body(feedbackDownloadDTO.getDownloadResultCsv());
  }
```



```java
/**
 * 下载符合筛选条件的反馈记录到 CSV 文件
 *
 * @param request 下载请求，包含筛选条件
 * @return 下载结果 DTO，包含 CSV 二进制流和 HTTP 响应头
 */
public FeedbackDownloadDTO downloadFeedbackRecords(FeedbackDownloadRequest request) {
  // 1. 根据筛选条件查询反馈记录
  List<FeedbackDownloadDao> feedbackDownloadRecords = feedbackCycleRepository.queryFeedbackDownloadData(
    request.getTopicIds(),
    request.getMessageId(),
    request.getSessionId(),
    request.getTeamCode(),
    request.getStatus(),
    request.getFeedback() == null || request.getFeedback().equals("All sessions")
      ? null
      : request.getFeedback(),
    request.getUserName(),
    request.getStartTime(),
    request.getEndTime());

  // 2. 生成 CSV 文件
  ByteArrayOutputStream csvStream = new ByteArrayOutputStream();
  try (PrintWriter writer = new PrintWriter(
      new OutputStreamWriter(csvStream, StandardCharsets.UTF_8))) {

    // 写 BOM（防止 Excel 打开乱码）
    writer.write('\uFEFF');

    // TODO 写入 CSV 头，避免写死
    String[] csvHeader = {"message_id", "session_id", "topic_name" ,"status", "comment", "question", "answer",
        "feedback_result", "feedback_reason", "user", "time"};
    writer.println(String.join(",", csvHeader));

    // 将 long 类型时间戳格式化为 yyyy-MM-dd HH:mm:ss
    DateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    // TODO 写入数据行，写入 CSV 字段进行转义处理，防止前端下载的 CSV 文件出现乱码
    for (FeedbackDownloadDao record : feedbackDownloadRecords) {
      writer.printf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n",
          CsvUtils.escapeCSVField(String.valueOf(record.getMessageId())),
          CsvUtils.escapeCSVField(String.valueOf(record.getSessionId())),
          CsvUtils.escapeCSVField(String.valueOf(record.getTopicName())),
          CsvUtils.escapeCSVField(record.getStatus()),
          CsvUtils.escapeCSVField(record.getComment()),
          CsvUtils.escapeCSVField(record.getQuestion()),
          CsvUtils.escapeCSVField(record.getAnswer()),
          CsvUtils.escapeCSVField(record.getFeedbackResult()),
          CsvUtils.escapeCSVField(record.getFeedbackReason()),
          CsvUtils.escapeCSVField(record.getUser()),
          dateFormat.format(record.getTime()));
    }
    writer.flush();
  }

  // 3. 创建输入流
  ByteArrayInputStream inputStream = new ByteArrayInputStream(csvStream.toByteArray());

  // 4. 获取时间戳用于文件名
  long timeStamp = System.currentTimeMillis();

  // 5. 设置 HTTP 响应头
  HttpHeaders headers = new HttpHeaders();
  // TODO 这里使用默认文件名
  String filename = String.format("feedback_tracking_%d.csv", timeStamp);

  // RFC 5987 编码：对文件名进行百分比编码
  String encodedFilename = URLEncoder.encode(filename, StandardCharsets.UTF_8)
      .replace("+", "%20"); // URLEncoder 将空格编码为 +，需转换为 %20

  // 使用 filename* 参数指定编码方式，格式：charset lang value
  String headerValue = String.format("attachment; filename*=UTF-8''%s", encodedFilename);

  // TODO 与其它地方的写入 CSV 文件头保持一致
  headers.set(HttpHeaders.CONTENT_DISPOSITION, headerValue);
  headers.setContentType(MediaType.parseMediaType("text/csv; charset=UTF-8"));

  return FeedbackDownloadDTO.builder()
      .headers(headers)
      .downloadResultCsv(new InputStreamResource(inputStream))
      .build();
}
```



```
  /**
   * 在feedback里创建 "新的" test session，一个feedback可以创建多个test session，但是只显示最后一个session result
   *
   */
  public TestSessionDTO createFeedbackTestSession(
      FeedbackCreateTestSessionVO feedbackCreateTestSessionVO) {

    String sessionName = String.format("Test On %s", feedbackCreateTestSessionVO.getTopicName());
    ChatBITopicEntityVO chatBITopicEntityVO = ChatBITopicEntityVO.builder()
        .id(feedbackCreateTestSessionVO.getTopicId())
        .name(feedbackCreateTestSessionVO.getTopicName())
        .build();
    SessionScope scope = SessionScope.builder()
        .chatBITopicList(List.of(chatBITopicEntityVO))
        .build();

    ChatSessionTab chatSessionTab = new ChatSessionTab();

    // 填充session 信息
    chatSessionTab.setUser(feedbackCreateTestSessionVO.getUserName());
    chatSessionTab.setModel(ModelType.gpt_4_1.getType());
    chatSessionTab.setCreateTime(System.currentTimeMillis());
    chatSessionTab.setUpdateTime(System.currentTimeMillis());
    chatSessionTab.setSessionScope(JsonUtils.toJsonWithOutNull(scope));

    chatSessionTab.setName(sessionName);
    chatSessionTab.setType(ChatSessionType.TEST_SESSION.getType());
    chatSessionTab.setResourceId(String.valueOf(feedbackCreateTestSessionVO.getTopicId()));

    TestSessionDTO testSessionDTO = new TestSessionDTO();

    testSessionDTO.setSessionId(null);

    return testSessionDTO;
  }
```



```
import io.swagger.v3.oas.annotations.media.Schema;
import lombok.Data;

/**
 * feedback创建test session，请求体
 */
@Data
@Schema(description = "feedback创建test session，请求体")
public class FeedbackCreateTestSessionVO {
  @Schema(description = "创建test session 的 feedback id ")
  private Long feedbackId;

  @Schema(description = "用户邮箱")
  private String userName;

  @Schema(description = "topic id，用来获取session的一些填充字段")
  private Long topicId;

  @Schema(description = "作用同topic id")
  private String topicName;
}
```



```
/**
 * 在feedback里创建test session，一个feedback可以创建多个test session，但是只显示最后一个session result
 *
 */
@PostMapping("/feedback_test")
public ResponseDTO<SessionCreateResponseVO> createFeedbackTestSession(
    @RequestBody FeedbackCreateTestSessionVO feedbackCreateTestSessionVO
) {
  log.info("在 feedback 内创建 test session：{}", feedbackCreateTestSessionVO);
  TestSessionDTO testSession =
      sessionService.createFeedbackTestSession(feedbackCreateTestSessionVO);

  SessionCreateResponseVO responseVO = new SessionCreateResponseVO();
  responseVO.setSessionId(testSession.getSessionId());
  return ResponseDTO.ok(responseVO);
}
```



```java
/**
 * 下载符合筛选条件的反馈记录到 CSV 文件
 *
 * @param request 下载请求，包含筛选条件
 * @return 下载结果 DTO，包含 CSV 二进制流和 HTTP 响应头
 */
public FeedbackDownloadDTO downloadFeedbackRecords(FeedbackDownloadRequest request) {
  // 1. 判断权限
  // 获取当前用户 email
  String currentUserEmail = DataSuiteAuthThreadLocal.getEmail();
  List<Long> resultTopicIds = request.getTopicIds();

  // 如果 team code 为 null，说明是指定 topic 下载，直接判断权限
  // 否则，team code 不为 null，topicIds 可以为 null 或者可以多选
  // 当 topicIds 为 null 时，说明用户没有选定任何topic，需要查询出该 team code 的所有 topic
  if (request.getTeamCode() != null && request.getTopicIds() == null) {
    resultTopicIds = chatbiTopicManager.findAllByTeamCode(request.getTeamCode());
  }

  // 调用 permissionCheckService 获取要下载的 topic list 中符合权限要求的 topic，
  // 并不是要求用户必须拥有列表中所有 topic 的权限，过滤后再执行查询
  List<Long> filterTopicIds =
      permissionCheckService.getOwnerOrProjectAdminOfTopic(resultTopicIds, currentUserEmail)
          .stream()
          .map(BaseDao::getId)
          .toList();

  // 2. 根据筛选条件查询反馈记录
  String feedbackResult = request.getFeedback();
  if (Objects.nonNull(request.getFeedback())) {
    feedbackResult = FeedbackType.fromValue(request.getFeedback()).getDbValue();
  }

  List<Map<String, Object>> feedbackRecords = feedbackCycleRepository
      .queryFeedbackDownloadData(
          filterTopicIds,
          request.getMessageId(),
          request.getSessionId(),
          request.getTeamCode(),
          request.getStatus() != null ? request.getStatus().stream().sorted()
              .collect(Collectors.joining(",")) : null,
          request.getFeedback() == null || request.getFeedback().equals("All sessions")
              ? null
              : feedbackResult,
          request.getUserName(),
          request.getStartTime(),
          request.getEndTime());

  // 将从数据库中查询出的哈希表映射为 FeedbackDownloadDao
  List<FeedbackDownloadDao> feedbackDownloadRecords = feedbackRecords.stream()
      .map(this::mapToFeedbackDownloadDao)
      .toList();

  // 3. 生成 CSV 文件
  ByteArrayOutputStream csvStream = new ByteArrayOutputStream();
  try (PrintWriter writer = new PrintWriter(
      new OutputStreamWriter(csvStream, StandardCharsets.UTF_8))) {

    // 写 BOM（防止 Excel 打开乱码）
    writer.write('\uFEFF');

    // 写入 CSV 头，避免写死
    String[] csvHeader = {"message_id", "session_id", "topic_name", "status", "comment",
        "question", "answer", "feedback_result", "feedback_reason", "user", "time"};
    writer.println(String.join(",", csvHeader));

    // 将 long 类型时间戳格式化为 yyyy-MM-dd HH:mm:ss
    DateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    // 写入数据行，写入 CSV 字段进行转义处理，防止前端下载的 CSV 文件出现乱码
    for (FeedbackDownloadDao record : feedbackDownloadRecords) {
      // 优化 question 和 answer 格式，避免因 dataScope 再次转义导致解析失败
      String questionContent = record.getQuestion();
      String answerContent = record.getAnswer();
      try {
        RequestMessageDTO questionDto = deserializeRequestMessage(record.getQuestion());
        if (questionDto != null && questionDto.getQuestion() != null) {
          questionContent = questionDto.getQuestion();
        }
      } catch (IllegalArgumentException ex) {
        log.warn(
            "Failed to parse question JSON for messageId {}. Fallback to raw text. payload={}",
            record.getMessageId(), record.getQuestion(), ex);
      }
      try {
        ResponseMessageDTO answerDto =
            JsonUtils.fromJsonString(record.getAnswer(), ResponseMessageDTO.class);
        if (answerDto != null && answerDto.toAnswer() != null) {
          answerContent = answerDto.toAnswer();
        }
      } catch (IllegalArgumentException ex) {
        log.warn("Failed to parse answer JSON for messageId {}. Fallback to raw text. payload={}",
            record.getMessageId(), record.getAnswer(), ex);
      }
      writer.printf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n",
          CsvUtils.escapeCSVField(String.valueOf(record.getMessageId())),
          CsvUtils.escapeCSVField(String.valueOf(record.getSessionId())),
          CsvUtils.escapeCSVField(String.valueOf(record.getTopicName())),
          CsvUtils.escapeCSVField(record.getStatus()),
          CsvUtils.escapeCSVField(record.getComment()),
          CsvUtils.escapeCSVField(questionContent),
          CsvUtils.escapeCSVField(answerContent),
          CsvUtils.escapeCSVField(record.getFeedbackResult()),
          CsvUtils.escapeCSVField(record.getFeedbackReason()),
          CsvUtils.escapeCSVField(record.getUser()),
          record.getTime() == null ? "" : dateFormat.format(record.getTime()));
    }
    writer.flush();
  }

  // 4. 创建输入流
  ByteArrayInputStream inputStream = new ByteArrayInputStream(csvStream.toByteArray());

  // 5. 获取时间戳用于文件名
  long timeStamp = System.currentTimeMillis();

  // 6. 设置 HTTP 响应头
  HttpHeaders headers = new HttpHeaders();
  // 这里使用默认文件名
  String filename = String.format("feedback_tracking_%d.csv", timeStamp);

  // RFC 5987 编码：对文件名进行百分比编码
  String encodedFilename = URLEncoder.encode(filename, StandardCharsets.UTF_8)
      .replace("+", "%20"); // URLEncoder 将空格编码为 +，需转换为 %20

  // 使用 filename* 参数指定编码方式，格式：charset lang value
  String headerValue = String.format("attachment; filename*=UTF-8''%s", encodedFilename);

  // 与其它地方的写入 CSV 文件头保持一致
  headers.set(HttpHeaders.CONTENT_DISPOSITION, headerValue);
  headers.setContentType(MediaType.parseMediaType("text/csv; charset=UTF-8"));

  return FeedbackDownloadDTO.builder()
      .headers(headers)
      .downloadResultCsv(new InputStreamResource(inputStream))
      .build();
}

/**
 * 将 question JSON 解析为 RequestMessageDTO。
 *
 * <p>
 * 若捕获到 dataScope 被再次转义（例如 "dataScope":"{\"tableUidList\":[]}"）的情况，会先调用
 * {@link #normalizeDataScope(String)} 尝试还原，再交给 Jackson 解析，避免无意义的反序列化异常。
 * </p>
 */
private RequestMessageDTO deserializeRequestMessage(String rawQuestionJson) {
  try {
    return JsonUtils.fromJsonString(rawQuestionJson, RequestMessageDTO.class);
  } catch (IllegalArgumentException parsingException) {
    String normalizedQuestion = normalizeDataScope(rawQuestionJson);
    if (!Objects.equals(normalizedQuestion, rawQuestionJson)) {
      return JsonUtils.fromJsonString(normalizedQuestion, RequestMessageDTO.class);
    }
    throw parsingException;
  }
}

/**
 * 处理 dataScope 被重复转义的情况，将字符串类型的 dataScope 字段重新还原为 JSON 对象。
 *
 * @param rawQuestionJson 原始的 question JSON
 * @return 如果成功还原则返回修正后的 JSON，否则返回原始字符串
 */
private String normalizeDataScope(String rawQuestionJson) {
  try {
    JsonNode rootNode = JsonUtils.getMapper().readTree(rawQuestionJson);
    JsonNode dataScopeNode = rootNode.get("dataScope");
    if (dataScopeNode != null && dataScopeNode.isTextual()) {
      String dataScopeJson = dataScopeNode.asText();
      if (!dataScopeJson.isEmpty()) {
        JsonNode fixedDataScope = JsonUtils.getMapper().readTree(dataScopeJson);
        ((ObjectNode) rootNode).set("dataScope", fixedDataScope);
        return JsonUtils.getMapper().writeValueAsString(rootNode);
      }
    }
  } catch (Exception ex) {
    log.warn("Failed to normalize dataScope in question JSON: {}", rawQuestionJson, ex);
  }
  return rawQuestionJson;
}
```
